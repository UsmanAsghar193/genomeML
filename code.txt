===== main.py =====
# Copy right (c) Xue Zhang, Weijia Xiao, and Wangxin Xiao 2020. All rights reserved.
# February 2020
# main.py
# importing packages
import time
import os
import logging
import argparse
from process_data import ProcessDataset
from DNN import DNN, make_folder

logging.basicConfig(format='%(levelname)s: %(name)s - %(message)s',
                    level=logging.INFO)
logger = logging.getLogger('main')

parser = argparse.ArgumentParser(description='DeepHE implementation!')
parser.add_argument("--expName", default="Experiment", type=str, help="This is used to form the file names of results! Default is 'Experiment'.")
parser.add_argument("--fold", default=4, type=int, help="The fold of nonessential genes versus essential genes. It's max value is <= #ne/#e. Default is 4.")
parser.add_argument("--embedF", default=3, type=int, help="0 for seq features; 1 for network features; other integers for both. Default is 3.")
parser.add_argument("--data_dir", default="data/", type=str, help="The dir for feature files. Default is data/")
parser.add_argument("--trainProp", default=0.8, type=float, help="The proportion of data used for training the model. Default is 0.8.")
parser.add_argument("--repeat", default=10, type=int, help="The times you want to run the experiments. Default is 10.")
parser.add_argument("--result_dir", default="results/", type=str, help="The dir to same results.")
parser.add_argument("--numHiddenLayer", default=3, type=int, help="The number of hidden layers in the DNN model.")

def main():
    args = parser.parse_args()
    expName = args.expName
    result_dir = args.result_dir
    
    # program start time
    start_time = time.time()

    # get features and build training/testing dataset
    pdata = ProcessDataset(data_dir=args.data_dir, trainProp=args.trainProp, ExpName=args.expName, embedF=args.embedF, fold=args.fold)

    # creating file to store evaluation statistics
    make_folder(result_dir)
    
    #Create result file and save some information of the experiments to it
    fn = os.path.join(result_dir, expName + '.txt')
    fw = open(fn, 'w')
    fw.write("Experiment Name: " + str(expName) + '\n\n')
    
    fw.write("Iteration" + "\t" + "ROC_AUC" + "\t" + "Avg. Precision" + "\t" +
                 "Sensitivity" + "\t" + "Specificity" + "\t" + "PPV" + "\t" + "Accuracy" + "\n")

    # dict to store evaluation statistics to calculate average values
    evaluationValueForAvg = {
        'roc_auc': 0.,
        'precision': 0.,
        'sensitivity': 0.,
        'specificity': 0.,
        'PPV': 0.,
        'accuracy': 0.
    }

    # build DNN model
    if os.path.exists(os.path.join(result_dir, expName + '_True_positives.txt')):
        os.remove(os.path.join(result_dir,expName + '_True_positives.txt'))
    if os.path.exists(os.path.join(result_dir, expName + '_False_positives.txt')):
        os.remove(os.path.join(result_dir, expName + '_False_positives.txt'))
    if os.path.exists(os.path.join(result_dir, expName + '_Thresholds.txt')):
        os.remove(os.path.join(result_dir, expName + '_Thresholds.txt'))

    f_tp = open(os.path.join(result_dir, expName + '_True_positives.txt'), 'a')
    f_fp = open(os.path.join(result_dir, expName + '_False_positives.txt'), 'a')
    f_th = open(os.path.join(result_dir, expName + '_Thresholds.txt'), 'a')

    for i in range(0, args.repeat):
        print('Iteration', i)
        model = DNN(pdata, f_tp, f_fp, f_th, expName, i, result_dir=args.result_dir)
        evaluationDict = model.getEvaluationStat()

        print(evaluationDict)

        saveEvaluation(evaluationDict, fw, i + 1)

        evaluationValueForAvg['roc_auc'] += evaluationDict['roc_auc']
        evaluationValueForAvg['precision'] += evaluationDict['precision']
        evaluationValueForAvg['sensitivity'] += evaluationDict['sensitivity']
        evaluationValueForAvg['specificity'] += evaluationDict['specificity']
        evaluationValueForAvg['PPV'] += evaluationDict['PPV']
        evaluationValueForAvg['accuracy'] += evaluationDict['accuracy']

    for value in evaluationValueForAvg:
        evaluationValueForAvg[value] = float(evaluationValueForAvg[value]) / args.repeat

    saveEvaluation(evaluationValueForAvg, fw, 'Avg.')
    fw.write("\n")
    fw.write("Number of training samples: " + str(evaluationDict['numTrain']) + '\n')
    fw.write("Number of validation samples: " + str(evaluationDict['numValidation']) + '\n')
    fw.write("Number of testing samples: " + str(evaluationDict['numTest']) + '\n')
    fw.write("Number of features: " + str(evaluationDict['numFeature']) + '\n')
    fw.write('Batch size:' + str(evaluationDict['batch_size']) + '\n')
    fw.write('Activation:' + str(evaluationDict['activation']) + '\n')
    fw.write('Dropout:' + str(evaluationDict['dropout']) + '\n')

    end_time = time.time()
    fw.write("Execution time: " + str(end_time - start_time) + " sec.")
    fw.close()
    # f_imp.close()

    f_tp.close()
    f_fp.close()
    f_th.close()


# writes the evaluation statistics
def saveEvaluation(evaluationDict, fw, iteration):
    fw.write(str(iteration) + "\t" + str(evaluationDict['roc_auc']) + "\t" +
                 str(evaluationDict['precision']) + '\t' + str(evaluationDict['sensitivity']) + '\t' +
                 str(evaluationDict['specificity']) + '\t' + str(evaluationDict['PPV']) + '\t' +
                 str(evaluationDict['accuracy']) + '\n')


if __name__ == "__main__":
    main()







===== process_data =====
##########################################################
#copy right (c) Xue Zhang 2020, all rights reserved.
#use sequence features and network embedding features
#for 2009 essential genes and 8430 nonessential genes
#########################################################
# process_data.py
import sys
import numpy as np
from sklearn.preprocessing import StandardScaler
import logging
import math
import os
import pickle

logging.basicConfig(format='%(levelname)s: %(name)s - %(message)s',
                    level=logging.INFO)
logger = logging.getLogger('ProcessDataset')


class ProcessDataset(object):
    def __init__(self, data_dir='data/', trainProp=0.8, ExpName='Balance-10percent', embedF=3, fold=4):
        super(ProcessDataset, self).__init__()
        
        self.data_dir = data_dir
        self.trainProp = trainProp
        self.ExpName = ExpName
        self.embedF = embedF
        self.fold = fold
        
    def getScaledData(self, dataMatrix):
        #return scaled dataset
        scaler = StandardScaler().fit(dataMatrix)
        return scaler.transform(dataMatrix)
    
    def partitionDataset(self):
        """
        embedF: 0 for sequence feature, 1 for embedding feature, and other values for the 
                combination of these two types of features
        """
        if self.embedF == 1:
            fn1 = os.path.join(self.data_dir, 'seqFeature.pkl')
            fn2 = os.path.join(self.data_dir, 'seqFeature.pkl')
        elif self.embedF == 0:
            fn1 = os.path.join(self.data_dir, 'seqFeature.pkl')
            fn2 = os.path.join(self.data_dir, 'seqFeature.pkl')
        else:
            fn1 = os.path.join(self.data_dir, 'seqFeature.pkl')
            fn2 = os.path.join(self.data_dir, 'seqFeature.pkl')
        
        if all([os.path.isfile(fn1), os.path.isfile(fn2)]):
            essGeneFeatTable = load_pickle(fn1)
            nessGeneFeatTable = load_pickle(fn2)
        else:
            sys.exit("Feature files {} and {} do not exist, please check!".format(fn1, fn2))
    
        trainData, validationData, testData = splitDataset(essGeneFeatTable, nessGeneFeatTable, self.trainProp, fold=self.fold)
        logger.info('trainData.shape={}*{}.'.format(trainData.shape[0],trainData.shape[1]))
        
        return trainData,validationData,testData

def splitDataset(essFeatTable, nessFeatTable, trainingProp, fold=4):
    """
    The size of nonessential genes is 4 fold of that of essential genes, so
    parameter fold should satisfy 1 <= fold <= 4
    """
    ness_num = math.ceil(essFeatTable.shape[0] * fold)
    nessTotal = nessFeatTable.shape[0]
    if ness_num <= nessTotal:
        nessTable = nessFeatTable[np.random.choice(nessTotal, ness_num, replace=False), :]
    else:
        nessTable = nessFeatTable[np.random.choice(nessTotal, ness_num, replace=True), :]
    
    # calculating training, validation, testing data portion
    validationProp, testingProp = float(1 - trainingProp) / 2, float(1 - trainingProp) / 2

    # shuffling the data to mix the data before splitting the dataset into training, validation and testing data
    np.random.shuffle(essFeatTable)
    np.random.shuffle(nessTable)

    # getting the shape of the reSized dataset to find the training, validation and testing size
    row1, col1 = essFeatTable.shape
    logger.info("essFeatTable has {} rows and {} columns.".format(row1, col1))
    
    row2, col2 = nessTable.shape
    logger.info("nessTable has {} rows and {} columns.".format(row2, col2))
    
    trainingSize = math.ceil(row1 * trainingProp)
    validationSize = int(row1 * validationProp)
    testingSize = row1 - trainingSize - validationSize

    etrainingData = essFeatTable[0:trainingSize, :]
    evalidationData = essFeatTable[trainingSize:(trainingSize + validationSize), :]
    etestingData = essFeatTable[(trainingSize + validationSize):, :]
    
    trainingSize = math.ceil(row2 * trainingProp)
    validationSize = int(row2 * validationProp)
    testingSize = row2 - trainingSize - validationSize
    
    netrainingData = nessTable[0:trainingSize, :]
    nevalidationData = nessTable[trainingSize:(trainingSize + validationSize), :]
    netestingData = nessTable[(trainingSize + validationSize):, :]
    
    trainingData = np.vstack((etrainingData, netrainingData))
    validationData = np.vstack((evalidationData, nevalidationData))
    testingData = np.vstack((etestingData, netestingData))
    
    np.random.shuffle(trainingData)
    np.random.shuffle(validationData)
    np.random.shuffle(testingData)
    
    logger.info("trainingData.shape={}*{}".format(trainingData.shape[0], trainingData.shape[1]))
    logger.info("validationData.shape={}*{}".format(validationData.shape[0], validationData.shape[1]))
    logger.info("testingData.shape={}*{}".format(testingData.shape[0], testingData.shape[1]))

    return trainingData, validationData, testingData
    
def save_pickle(fn, data):
    with open(fn, 'wb') as pickle_out:
        pickle.dump(data, pickle_out)
        
def load_pickle(fn):
    with open(fn, 'rb') as pickle_in:
        data = pickle.load(pickle_in)
    return data





===== DNN.py =====
# Copy right (c) Xue Zhang and Weijia Xiao 2020. All rights reserved.
#
#
# dnn.py
import os
import math
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn import metrics
import logging
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.optimizers import Adam
# from keras.utils import np_utils
from keras.utils import to_categorical

from keras.callbacks import ModelCheckpoint, EarlyStopping


logging.basicConfig(format='%(levelname)s: %(name)s - %(message)s',
                    level=logging.INFO)
logger = logging.getLogger('DNN')


# parameter dictionary
paramDict = {
    'epoch': 200,
    'batchSize': 32,
    'dropOut': 0.2,
    'loss': 'binary_crossentropy',
    'metrics': ['accuracy'],
    'activation1': 'relu',
    'activation2': 'sigmoid',
    'monitor': 'val_accuracy',
    'save_best_only': True,
    'mode': 'max'
}

class_weight = {0: 1.0, 1: 4.0}

optimizerDict = {
    'adam': Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),
}

hl = [128, 256, 512, 1024, 1024, 1024,1024, 1024, 1024, 1024];

def make_folder(folder):
    if not os.path.isdir(folder):
        os.mkdir(folder)
        logger.info('{} is created.'.format(folder))
    else:
        logger.info('{} is already there.'.format(folder))


class DNN(object):

    def __init__(self, pdata, f_tp, f_fp, f_th, expName, iteration, numHidden=3, result_dir='node2vec/results/'):
        super(DNN, self).__init__()
        self.pdata = pdata
        self.expName = expName
        self.model_dir = os.path.join(result_dir, 'model')
        make_folder(self.model_dir)

        self.evaluationInfo = dict()

        self.trainingData, self.validationData, self.testingData = pdata.partitionDataset()

        X_train, Y_train = separateDataAndClassLabel(self.trainingData)
        X_valid, Y_valid = separateDataAndClassLabel(self.validationData)
        X_test, Y_test = separateDataAndClassLabel(self.testingData)

        #
        X_train = pdata.getScaledData(X_train)
        X_valid = pdata.getScaledData(X_valid)
        X_test = pdata.getScaledData(X_test)

        self.numberOfClasses = encodeClassLabel(Y_train)
        self.numberOfFeature = X_train.shape[1]

        # reshaping class labels
        Y_train_reshaped = to_categorical(Y_train, self.numberOfClasses)
        Y_valid_reshaped = to_categorical(Y_valid, self.numberOfClasses)
        Y_test_reshaped = to_categorical(Y_test, self.numberOfClasses)

        self.dataDict = {
            'train': X_train,
            'trainLabel': Y_train_reshaped,
            'valid': X_valid,
            'validLabel': Y_valid_reshaped,
            'test': X_test,
            'testLabel': Y_test_reshaped
        }

        self.evaluationInfo = buildModel(self.dataDict, self.numberOfFeature, self.numberOfClasses,
                                               f_tp, f_fp, f_th, expName, iteration, self.model_dir, result_dir)
        self.evaluationInfo['numTrain'] = X_train.shape[0]
        self.evaluationInfo['numTest'] = X_test.shape[0]
        self.evaluationInfo['numValidation'] = X_valid.shape[0]
        self.evaluationInfo['numFeature'] = self.numberOfFeature

    def getEvaluationStat(self):
        return self.evaluationInfo

# returns the TP, TN, FP and FN values
def getTPTNValues(test, testPred):
    TP, TN, FP, FN = 0, 0, 0, 0
    for i in range(len(testPred)):
        if test[i] == testPred[i] == 1:
            TP += 1
        elif testPred[i] == 1 and test[i] != testPred[i]:
            FP += 1
        elif test[i] == testPred[i] == 0:
            TN += 1
        elif testPred[i] == 0 and test[i] != testPred[i]:
            FN += 1

    return TP, TN, FP, FN


# separating feature matrix and class label
def separateDataAndClassLabel(dataMatrix):
    featureMatrix = dataMatrix[:, :(dataMatrix.shape[1] - 1)]
    classLabelMatrix = dataMatrix[:, -1]

    return featureMatrix, classLabelMatrix


# returns the number of classes and encode it
def encodeClassLabel(classLabel):
    labelEncoder = LabelEncoder().fit(classLabel)
    labels = labelEncoder.transform(classLabel)
    classes = list(labelEncoder.classes_)
    return len(classes)

# building the DNN model and run with the data, returns a list of metrics
def buildModel(dataDict, numFeat, numberOfClasses, f_tp, f_fp, f_th, expName, iteration, model_dir, result_dir):
    trainData = dataDict['train']
    trainLabel = dataDict['trainLabel']
    validData = dataDict['valid']
    validLabel = dataDict['validLabel']
    testData = dataDict['test']
    testLabel = dataDict['testLabel']

    # building NN model
    model = Sequential()
    model.add(Dense(hl[0], activation = paramDict['activation1'], input_shape = (numFeat, )))
    model.add(Dropout(paramDict['dropOut']))
    for i in range(1, numHidden):
        if i < len(hl):
            model.add(Dense(hl[i], activation = paramDict['activation1']))
            model.add(Dropout(paramDict['dropOut']))
        else:
            model.add(Dense(1024, activation = paramDict['activation1']))
            model.add(Dropout(paramDict['dropOut']))
    
    model.add(Dense(numberOfClasses, activation=paramDict['activation2']))

    model.compile(optimizer=optimizerDict['adam'],
                  loss=paramDict['loss'],
                  metrics=paramDict['metrics'])
    
    # saving best model by validation accuracy
    filePath = os.path.join(model_dir, expName + str(iteration) + '_weights.best.hdf5')
    checkpointer = ModelCheckpoint(filepath=filePath, verbose=0, monitor=paramDict['monitor'], save_best_only=True)
    earlystopper = EarlyStopping(paramDict['monitor'], patience=15, verbose=1)

    # fit the model to the training data and verify with validation data
    model.fit(trainData, trainLabel,
              epochs=paramDict['epoch'],
              callbacks=[checkpointer, earlystopper],
              batch_size=paramDict['batchSize'],
              shuffle=True,
              verbose=1,
              validation_data=(validData, validLabel), class_weight = class_weight)

    # load best model and compile
    model.load_weights(filePath)
    model.compile(optimizer=optimizerDict['adam'],
                  loss=paramDict['loss'],
                  metrics=paramDict['metrics'])
    
    # serialize model to JSON (save the model structure in order to use the saved weights)
    #one time save
    fn = os.path.join(model_dir, 'model3.json')
    if not os.path.isfile(fn):
        model_json = model.to_json()
        with open(fn, 'w') as json_file:
            json_file.write(model_json)
            
    #save model for later use (including model structure and weights)
    model_file = os.path.join(model_dir, expName + str(iteration) + '_model.h5')
    model.save(model_file)
    
    # evaluation scores
    roc_auc = metrics.roc_auc_score(testLabel, model.predict(testData))
    
    #precision here is the auc of precision-recall curve
    precision = metrics.average_precision_score(testLabel, model.predict(testData))

    # get predicted class label
    probs = model.predict_proba(testData)
    testPredLabel = model.predict(testData)
    true_y = list()
    for y_i in range(len(testLabel)):
        true_y.append(testLabel[y_i][1])
    probs = probs[:, 1]

    fpr, tpr, threshold = metrics.roc_curve(true_y, probs)

    for i in range(len(fpr)):
        f_fp.write(str(fpr[i]) + '\t')
    f_fp.write('\n')

    for i in range(len(tpr)):
        f_tp.write(str(tpr[i]) + '\t')
    f_tp.write('\n')

    for i in range(len(threshold)):
        f_th.write(str(threshold[i]) + '\t')
    f_th.write('\n')
    
    #save precision, recall, and thresholds for PR curve plot
    p0, r0, t0 = metrics.precision_recall_curve(true_y, probs)
    fnp0 = os.path.join(result_dir, expName + '_precision.txt')
    fnr0 = os.path.join(result_dir, expName + '_recall.txt')
    fnt0 = os.path.join(result_dir, expName + '_PR_threshold.txt')
    with open(fnp0, 'a') as f0:
        for i in range(len(p0)):
            f0.write(str(p0[i]) + '\t')
        f0.write('\n')
            
    with open(fnr0, 'a') as f0:
        for i in range(len(r0)):
            f0.write(str(r0[i]) + '\t')
        f0.write('\n')
    
    with open(fnt0, 'a') as f0:
        for i in range(len(t0)):
            f0.write(str(t0[i]) + '\t')
        f0.write('\n')
    
    # convert back class label from categorical to integer label
    testLabelRev = np.argmax(testLabel, axis=1)
    testPredLabelRev = np.argmax(testPredLabel, axis=1)

    # get TP, TN, FP, FN to calculate sensitivity, specificity, PPV and accuracy
    TP, TN, FP, FN = getTPTNValues(testLabelRev, testPredLabelRev)

    sensitivity = float(TP) / float(TP + FN)
    specificity = float(TN) / float(TN + FP)
    PPV = float(TP) / float(TP + FP)
    accuracy = float(TP + TN) / float(TP + FP + FN + TN)

    # dictionary to store evaluation stat
    evaluationInfo = {
        'roc_auc': roc_auc,
        'precision': precision,
        'sensitivity': sensitivity,
        'specificity': specificity,
        'PPV': PPV,
        'accuracy': accuracy,
        'batch_size': paramDict['batchSize'],
        'activation': paramDict['activation2'],
        'dropout': paramDict['dropOut']
    }

    return evaluationInfo
    




===== csv_t0_pkl_converter.py =====
import numpy as np
import pandas as pd

csv_file_path = "data/nodes_seq_feat.csv"
df = pd.read_csv(csv_file_path)
pickle_file_path = 'seqFeature.pkl'
# Save the DataFrame to a pickle file
df.to_pickle(pickle_file_path)

print(f"CSV file '{csv_file_path}' converted to pickle file '{pickle_file_path}'")


===== read_pkl_file.py =====
import pandas as pd


pickle_file_path = 'seqFeature.pkl'

# Loading the pickle file into a pandas DataFrame
df = pd.read_pickle(pickle_file_path)
print(df)